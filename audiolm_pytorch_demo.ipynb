{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n337KoD2om3L",
    "outputId": "97ada0c6-f21c-483e-d63d-08abddd49004",
    "ExecuteTime": {
     "end_time": "2023-10-04T06:47:02.629585900Z",
     "start_time": "2023-10-04T06:46:59.314746500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct  4 14:47:00 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.116.03   Driver Version: 525.116.03   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:04:00.0 Off |                  N/A |\r\n",
      "| 51%   27C    P8    21W / 350W |   4497MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |                  N/A |\r\n",
      "| 51%   29C    P8    24W / 350W |      3MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:07:00.0 Off |                  N/A |\r\n",
      "| 54%   30C    P8    27W / 350W |      3MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:08:00.0 Off |                  N/A |\r\n",
      "| 51%   29C    P8    24W / 350W |      3MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:0C:00.0 Off |                  N/A |\r\n",
      "| 54%   29C    P8    18W / 350W |      3MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:0D:00.0 Off |                  N/A |\r\n",
      "| 49%   29C    P8    22W / 350W |      3MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   6  NVIDIA GeForce ...  Off  | 00000000:0F:00.0 Off |                  N/A |\r\n",
      "| 43%   32C    P8    21W / 350W |      3MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      3598      G   /usr/bin/X                          6MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!conda activate audioWatermark\n",
    "# If this doesn't work, there's no GPU available or detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLJAcUHpvmp4",
    "outputId": "95bcda95-a484-40c6-e5a7-47f4378759a8",
    "ExecuteTime": {
     "end_time": "2023-10-04T06:35:22.379153900Z",
     "start_time": "2023-10-04T06:30:37.962464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting audiolm-pytorch\r\n",
      "  Downloading audiolm_pytorch-1.5.1-py3-none-any.whl (41 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.5/41.5 kB\u001B[0m \u001B[31m127.8 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting accelerate\r\n",
      "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m258.1/258.1 kB\u001B[0m \u001B[31m574.5 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting fairseq\r\n",
      "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.6/9.6 MB\u001B[0m \u001B[31m8.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Installing backend dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting einops>=0.6.1\r\n",
      "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.6/44.6 kB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting lion-pytorch\r\n",
      "  Using cached lion_pytorch-0.1.2-py3-none-any.whl (4.4 kB)\r\n",
      "Collecting ema-pytorch>=0.2.2\r\n",
      "  Using cached ema_pytorch-0.2.3-py3-none-any.whl (4.4 kB)\r\n",
      "Collecting local-attention>=1.8.4\r\n",
      "  Using cached local_attention-1.8.6-py3-none-any.whl (8.1 kB)\r\n",
      "Collecting encodec\r\n",
      "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.7/3.7 MB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting beartype>=0.16.1\r\n",
      "  Downloading beartype-0.16.2-py3-none-any.whl (811 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m811.8/811.8 kB\u001B[0m \u001B[31m6.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: joblib in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from audiolm-pytorch) (1.1.1)\r\n",
      "Collecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m11.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: tqdm in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from audiolm-pytorch) (4.64.1)\r\n",
      "Requirement already satisfied: transformers in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from audiolm-pytorch) (4.24.0)\r\n",
      "Requirement already satisfied: torch>=1.12 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from audiolm-pytorch) (1.12.1)\r\n",
      "Requirement already satisfied: scikit-learn in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from audiolm-pytorch) (1.2.1)\r\n",
      "Collecting torchaudio\r\n",
      "  Downloading torchaudio-2.0.2-cp310-cp310-manylinux1_x86_64.whl (4.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.4/4.4 MB\u001B[0m \u001B[31m11.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting vector-quantize-pytorch>=1.7.0\r\n",
      "  Using cached vector_quantize_pytorch-1.8.1-py3-none-any.whl (15 kB)\r\n",
      "Requirement already satisfied: typing_extensions in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (4.4.0)\r\n",
      "Requirement already satisfied: huggingface-hub in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from accelerate->audiolm-pytorch) (0.10.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from accelerate->audiolm-pytorch) (1.23.5)\r\n",
      "Requirement already satisfied: pyyaml in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from accelerate->audiolm-pytorch) (6.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from accelerate->audiolm-pytorch) (22.0)\r\n",
      "Requirement already satisfied: psutil in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from accelerate->audiolm-pytorch) (5.9.0)\r\n",
      "Collecting sacrebleu>=1.4.12\r\n",
      "  Using cached sacrebleu-2.3.1-py3-none-any.whl (118 kB)\r\n",
      "Requirement already satisfied: regex in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from fairseq->audiolm-pytorch) (2022.7.9)\r\n",
      "Collecting cython\r\n",
      "  Using cached Cython-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "Collecting hydra-core<1.1,>=1.0.7\r\n",
      "  Using cached hydra_core-1.0.7-py3-none-any.whl (123 kB)\r\n",
      "Collecting bitarray\r\n",
      "  Downloading bitarray-2.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m286.5/286.5 kB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting omegaconf<2.1\r\n",
      "  Using cached omegaconf-2.0.6-py3-none-any.whl (36 kB)\r\n",
      "Requirement already satisfied: cffi in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from fairseq->audiolm-pytorch) (1.15.1)\r\n",
      "Collecting torch>=1.12\r\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m619.9/619.9 MB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cufft-cu11==10.9.0.58\r\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m168.4/168.4 MB\u001B[0m \u001B[31m7.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: sympy in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (1.11.1)\r\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\r\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m317.1/317.1 MB\u001B[0m \u001B[31m4.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-curand-cu11==10.2.10.91\r\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m54.6/54.6 MB\u001B[0m \u001B[31m11.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\r\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m557.1/557.1 MB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu11==11.7.91\r\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m98.6/98.6 kB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\r\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m849.3/849.3 kB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\r\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m102.6/102.6 MB\u001B[0m \u001B[31m9.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: jinja2 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (3.1.2)\r\n",
      "Requirement already satisfied: filelock in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (3.9.0)\r\n",
      "Collecting triton==2.0.0\r\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.3/63.3 MB\u001B[0m \u001B[31m11.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: networkx in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (2.8.4)\r\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\r\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.8/11.8 MB\u001B[0m \u001B[31m15.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\r\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m173.2/173.2 MB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.0/21.0 MB\u001B[0m \u001B[31m14.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-nccl-cu11==2.14.3\r\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m177.1/177.1 MB\u001B[0m \u001B[31m7.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: setuptools in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.12->audiolm-pytorch) (65.6.3)\r\n",
      "Requirement already satisfied: wheel in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.12->audiolm-pytorch) (0.38.4)\r\n",
      "Collecting lit\r\n",
      "  Downloading lit-17.0.2.tar.gz (154 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.6/154.6 kB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Installing backend dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting cmake\r\n",
      "  Downloading cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.0/26.0 MB\u001B[0m \u001B[31m13.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from scikit-learn->audiolm-pytorch) (2.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from scikit-learn->audiolm-pytorch) (1.10.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from transformers->audiolm-pytorch) (0.11.4)\r\n",
      "Requirement already satisfied: requests in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from transformers->audiolm-pytorch) (2.28.1)\r\n",
      "Collecting antlr4-python3-runtime==4.8\r\n",
      "  Using cached antlr4-python3-runtime-4.8.tar.gz (112 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: tabulate>=0.8.9 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch) (0.8.10)\r\n",
      "Requirement already satisfied: lxml in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch) (4.9.1)\r\n",
      "Collecting portalocker\r\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\r\n",
      "Requirement already satisfied: colorama in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch) (0.4.6)\r\n",
      "Requirement already satisfied: pycparser in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from cffi->fairseq->audiolm-pytorch) (2.21)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from jinja2->torch>=1.12->audiolm-pytorch) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from requests->transformers->audiolm-pytorch) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from requests->transformers->audiolm-pytorch) (2022.12.7)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from requests->transformers->audiolm-pytorch) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lutianhe/anaconda3/lib/python3.10/site-packages (from requests->transformers->audiolm-pytorch) (3.4)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/lutianhe/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch>=1.12->audiolm-pytorch) (1.2.1)\r\n",
      "Building wheels for collected packages: encodec, fairseq, antlr4-python3-runtime, lit\r\n",
      "  Building wheel for encodec (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45761 sha256=8abcf05685063ebbd91032779e9f4aeb16e41c4f1b7c315b5a8e08322a5e6a19\r\n",
      "  Stored in directory: /home/lutianhe/.cache/pip/wheels/ce/7b/9e/791357a98076b1d06f7113db0cb70a217c3feb31962366ab9d\r\n",
      "  Building wheel for fairseq (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=10394303 sha256=47222ad24a1d65f2369c4894e71d607f617453e6bea7ea61088ecd396651e1c7\r\n",
      "  Stored in directory: /home/lutianhe/.cache/pip/wheels/9e/bd/73/c2508b87720b819fd03cbcf93bd39c18890fc60604a13ad6dc\r\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=b4d7cade889e49f25fe048ab71e114c55cfc835d3ed06dd3c5e72071387c0120\r\n",
      "  Stored in directory: /home/lutianhe/.cache/pip/wheels/ff/00/a9/e529ac6bfcd1da5193da71adc06168186f58f70733e1dfead0\r\n",
      "  Building wheel for lit (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for lit: filename=lit-17.0.2-py3-none-any.whl size=93257 sha256=5e34193e06cf3291db6df66bb62fbe78a3c29c1eb29042403c98b9a97f88a3de\r\n",
      "  Stored in directory: /home/lutianhe/.cache/pip/wheels/33/33/07/ab2cceb88bc840d7723e8450115a751296759eeb6955420a64\r\n",
      "Successfully built encodec fairseq antlr4-python3-runtime lit\r\n",
      "Installing collected packages: sentencepiece, lit, cmake, bitarray, antlr4-python3-runtime, portalocker, omegaconf, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, einops, cython, beartype, sacrebleu, nvidia-cusolver-cu11, nvidia-cudnn-cu11, hydra-core, triton, torch, torchaudio, vector-quantize-pytorch, local-attention, lion-pytorch, fairseq, encodec, ema-pytorch, accelerate, audiolm-pytorch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.12.1\r\n",
      "    Uninstalling torch-1.12.1:\r\n",
      "      Successfully uninstalled torch-1.12.1\r\n",
      "Successfully installed accelerate-0.23.0 antlr4-python3-runtime-4.8 audiolm-pytorch-1.5.1 beartype-0.16.2 bitarray-2.8.2 cmake-3.27.6 cython-3.0.2 einops-0.7.0 ema-pytorch-0.2.3 encodec-0.1.1 fairseq-0.12.2 hydra-core-1.0.7 lion-pytorch-0.1.2 lit-17.0.2 local-attention-1.8.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.3.1 sentencepiece-0.1.99 torch-2.0.1 torchaudio-2.0.2 triton-2.0.0 vector-quantize-pytorch-1.8.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install audiolm-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuNcsDJsvQwh"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Includes:\n",
    "\n",
    "- How to generate a placeholder dataset if you haven't already, just the basics to run \"training\" e2e on a tiny dataset\n",
    "- How to download a dataset from OpenSLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBxNK5cKW--_"
   },
   "source": [
    "### Imports & paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OrNeKngVVM0L",
    "ExecuteTime": {
     "end_time": "2023-10-04T06:47:09.536007300Z",
     "start_time": "2023-10-04T06:47:09.531019200Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import wave\n",
    "import struct\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer, HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "# define all dataset paths, checkpoints, etc\n",
    "dataset_folder = \"placeholder_dataset\"\n",
    "soundstream_ckpt = \"results/soundstream.8.pt\" # this can change depending on number of steps\n",
    "hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
    "hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pA56YODZXBtf"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6nnPceFWwedh",
    "ExecuteTime": {
     "end_time": "2023-10-04T06:47:13.689719800Z",
     "start_time": "2023-10-04T06:47:13.241226100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lutianhe/audioWatermark/placeholder_dataset\n"
     ]
    }
   ],
   "source": [
    "# Placeholder data generation\n",
    "def get_sinewave(freq=440.0, duration_ms=200, volume=1.0, sample_rate=44100.0):\n",
    "  # code adapted from https://stackoverflow.com/a/33913403\n",
    "  audio = []\n",
    "  num_samples = duration_ms * (sample_rate / 1000.0)\n",
    "  for x in range(int(num_samples)):\n",
    "    audio.append(volume * math.sin(2 * math.pi * freq * (x / sample_rate)))\n",
    "  return audio\n",
    "\n",
    "def save_wav(file_name, audio, sample_rate=44100.0):\n",
    "  # Open up a wav file\n",
    "  wav_file=wave.open(file_name,\"w\")\n",
    "  # wav params\n",
    "  nchannels = 1\n",
    "  sampwidth = 2\n",
    "  # 44100 is the industry standard sample rate - CD quality.  If you need to\n",
    "  # save on file size you can adjust it downwards. The stanard for low quality\n",
    "  # is 8000 or 8kHz.\n",
    "  nframes = len(audio)\n",
    "  comptype = \"NONE\"\n",
    "  compname = \"not compressed\"\n",
    "  wav_file.setparams((nchannels, sampwidth, sample_rate, nframes, comptype, compname))\n",
    "  # WAV files here are using short, 16 bit, signed integers for the \n",
    "  # sample size.  So we multiply the floating point data we have by 32767, the\n",
    "  # maximum value for a short integer.  NOTE: It is theortically possible to\n",
    "  # use the floating point -1.0 to 1.0 data directly in a WAV file but not\n",
    "  # obvious how to do that using the wave module in python.\n",
    "  for sample in audio:\n",
    "      wav_file.writeframes(struct.pack('h', int( sample * 32767.0 )))\n",
    "  wav_file.close()\n",
    "  return\n",
    "\n",
    "def make_placeholder_dataset():\n",
    "  print(os.path.abspath(dataset_folder))\n",
    "  # Make a placeholder dataset with a few .wav files that you can \"train\" on, just to verify things work e2e\n",
    "  if os.path.isdir(dataset_folder):\n",
    "    return\n",
    "  os.makedirs(dataset_folder)\n",
    "  save_wav(f\"{dataset_folder}/example.wav\", get_sinewave())\n",
    "  save_wav(f\"{dataset_folder}/example2.wav\", get_sinewave(duration_ms=500))\n",
    "  os.makedirs(f\"{dataset_folder}/subdirectory\")\n",
    "  save_wav(f\"{dataset_folder}/subdirectory/example.wav\", get_sinewave(freq=330.0))\n",
    "\n",
    "make_placeholder_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jwYCbFpHvmRI",
    "ExecuteTime": {
     "end_time": "2023-10-04T06:53:50.977220100Z",
     "start_time": "2023-10-04T06:53:50.938350700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get actual dataset. Uncomment this if you want to try training on real data\n",
    "\n",
    "# full dataset: https://www.openslr.org/12\n",
    "# We'll use https://us.openslr.org/resources/12/dev-clean.tar.gz development set, \"clean\" speech.\n",
    "# We *should* train on, well, training, but this is just to demo running things end-to-end at all so I just picked a small clean set.\n",
    "\n",
    "url = \"https://us.openslr.org/resources/12/dev-clean.tar.gz\"\n",
    "filename = \"dev-clean\"\n",
    "# filename = \"dev-other\"\n",
    "filename_targz = filename + \".tar.gz\"\n",
    "if not os.path.isfile(filename_targz):\n",
    "  urllib.request.urlretrieve(url, filename_targz)\n",
    "if not os.path.isdir(filename):\n",
    "  # open file\n",
    "  with tarfile.open(filename_targz) as t:\n",
    "    t.extractall(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYcI0aXEwuxR"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have a dataset, we can train AudioLM.\n",
    "\n",
    "**Note**: do NOT type \"y\" to overwrite previous experiments/ checkpoints when running through the cells here unless you're ready to the entire results folder! Otherwise you will end up erasing things (e.g. you train SoundStream first, and if you choose \"overwrite\" then you lose the SoundStream checkpoint when you then train SemanticTransformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7GiyBcBWiZV"
   },
   "source": [
    "### SoundStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nGU0OZiOwPEO",
    "outputId": "21dd959c-6458-4477-8403-cf810166f38d",
    "ExecuteTime": {
     "end_time": "2023-10-04T06:53:53.714711500Z",
     "start_time": "2023-10-04T06:53:53.091261200Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "only one Trainer can be instantiated at a time for training",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m soundstream \u001B[38;5;241m=\u001B[39m SoundStream(\n\u001B[1;32m      2\u001B[0m     codebook_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1024\u001B[39m,\n\u001B[1;32m      3\u001B[0m     rq_num_quantizers \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m,\n\u001B[1;32m      4\u001B[0m )\n\u001B[0;32m----> 6\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mSoundStreamTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43msoundstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfolder\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_accum_every\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m         \u001B[49m\u001B[38;5;66;43;03m# effective batch size of 32\u001B[39;49;00m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_max_length\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m320\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_results_every\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_model_every\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_train_steps\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m9\u001B[39;49m\n\u001B[1;32m     15\u001B[0m \u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# adjusting save_*_every variables for the same reason\u001B[39;00m\n\u001B[1;32m     19\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m<@beartype(audiolm_pytorch.trainer.SoundStreamTrainer.__init__) at 0x7f33d71d0dc0>:463\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_128088896, __beartype_object_139860633948608, __beartype_object_94927120, __beartype_object_125852784, *args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/audioWatermark/audiolm_pytorch/trainer.py:178\u001B[0m, in \u001B[0;36mSoundStreamTrainer.__init__\u001B[0;34m(self, soundstream, num_train_steps, batch_size, data_max_length, data_max_length_seconds, folder, train_dataloader, val_dataloader, lr, grad_accum_every, wd, max_grad_norm, discr_max_grad_norm, save_results_every, save_model_every, log_losses_every, results_folder, valid_frac, random_split_seed, use_ema, ema_beta, ema_update_after_step, ema_update_every, apply_grad_penalty_every, dl_num_workers, accelerator, accelerate_kwargs, dataloader_drop_last, split_batches, use_lion, force_clear_prev_results)\u001B[0m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03mInitialize with a SoundStream instance and either a folder containing audio data or\u001B[39;00m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;124;03mtrain/val DataLoader instances.\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m--> 178\u001B[0m \u001B[43mcheck_one_trainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m accelerator:\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator \u001B[38;5;241m=\u001B[39m accelerator\n",
      "File \u001B[0;32m~/audioWatermark/audiolm_pytorch/trainer.py:60\u001B[0m, in \u001B[0;36mcheck_one_trainer\u001B[0;34m()\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_one_trainer\u001B[39m():\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mglobal\u001B[39;00m ONE_TRAINER_INSTANTIATED\n\u001B[0;32m---> 60\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ONE_TRAINER_INSTANTIATED, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124monly one Trainer can be instantiated at a time for training\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     61\u001B[0m     ONE_TRAINER_INSTANTIATED \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: only one Trainer can be instantiated at a time for training"
     ]
    }
   ],
   "source": [
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "trainer = SoundStreamTrainer(\n",
    "    soundstream,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 8,         # effective batch size of 32\n",
    "    data_max_length = 320 * 32,\n",
    "    save_results_every = 2,\n",
    "    save_model_every = 4,\n",
    "    num_train_steps = 9\n",
    ").cuda()\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqjN28L4Wc5Q"
   },
   "source": [
    "### SemanticTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgd962eSvDzS",
    "outputId": "b0550cde-0c8b-4a39-f896-f6f813f50f8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
      "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
      "0: loss: 6.648584365844727\n",
      "0: valid loss 5.763116359710693\n",
      "0: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "# hubert checkpoints can be downloaded at\n",
    "# https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n",
    "if not os.path.isdir(\"hubert\"):\n",
    "  os.makedirs(\"hubert\")\n",
    "if not os.path.isfile(hubert_ckpt):\n",
    "  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n",
    "  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n",
    "if not os.path.isfile(hubert_quantizer):\n",
    "  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n",
    "  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")\n",
    "\n",
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = f'./{hubert_ckpt}',\n",
    "    kmeans_path = f'./{hubert_quantizer}'\n",
    ")\n",
    "\n",
    "semantic_transformer = SemanticTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    dim = 1024,\n",
    "    depth = 6\n",
    ").cuda()\n",
    "\n",
    "\n",
    "trainer = SemanticTransformerTrainer(\n",
    "    transformer = semantic_transformer,\n",
    "    wav2vec = wav2vec,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eEvIzhEWwRz"
   },
   "source": [
    "### CoarseTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LeWmaNHzzY9",
    "outputId": "7e7ecb3b-f59e-4d18-c8c9-64762e9b43fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
      "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
      "0: loss: 63.983970642089844\n",
      "0: valid loss 63.398582458496094\n",
      "0: saving model to results\n",
      "1: loss: 65.85967254638672\n",
      "2: loss: 62.4722900390625\n",
      "2: valid loss 50.01605987548828\n",
      "3: loss: 11.735434532165527\n",
      "4: loss: 3.976104497909546\n",
      "4: valid loss 46.094608306884766\n",
      "4: saving model to results\n",
      "5: loss: 58.27140426635742\n",
      "6: loss: 41.68347930908203\n",
      "6: valid loss 45.54595184326172\n",
      "7: loss: 2.2387890815734863\n",
      "8: loss: 0.4718627631664276\n",
      "8: valid loss 39.10848617553711\n",
      "8: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = f'./{hubert_ckpt}',\n",
    "    kmeans_path = f'./{hubert_quantizer}'\n",
    ")\n",
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "soundstream.load(f\"./{soundstream_ckpt}\")\n",
    "\n",
    "coarse_transformer = CoarseTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    codebook_size = 1024,\n",
    "    num_coarse_quantizers = 3,\n",
    "    dim = 512,\n",
    "    depth = 6\n",
    ")\n",
    "\n",
    "trainer = CoarseTransformerTrainer(\n",
    "    transformer = coarse_transformer,\n",
    "    codec = soundstream,\n",
    "    wav2vec = wav2vec,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    save_results_every = 2,\n",
    "    save_model_every = 4,\n",
    "    num_train_steps = 9\n",
    ")\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRvj7qOJWzmw"
   },
   "source": [
    "### FineTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRaEhRRKWg8F",
    "outputId": "7cc166c4-c8e9-45ef-8293-8f5381c2d3af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
      "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
      "0: loss: 70.90608215332031\n",
      "0: valid loss 65.99951171875\n",
      "0: saving model to results\n",
      "1: loss: 43.6014289855957\n",
      "2: loss: 8.300681114196777\n",
      "3: loss: 61.23375701904297\n",
      "4: loss: 63.34052276611328\n",
      "5: loss: 2.010118246078491\n",
      "6: loss: 56.52588653564453\n",
      "7: loss: 0.5423888564109802\n",
      "8: loss: 0.005095238331705332\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "soundstream.load(f\"./{soundstream_ckpt}\")\n",
    "\n",
    "fine_transformer = FineTransformer(\n",
    "    num_coarse_quantizers = 3,\n",
    "    num_fine_quantizers = 5,\n",
    "    codebook_size = 1024,\n",
    "    dim = 512,\n",
    "    depth = 6\n",
    ")\n",
    "\n",
    "trainer = FineTransformerTrainer(\n",
    "    transformer = fine_transformer,\n",
    "    codec = soundstream,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 9\n",
    ")\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoHgkgA3XKXH"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzghrux5WinW",
    "outputId": "9dd39f7f-0046-4a5f-826e-a442345987af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating semantic:   0%|          | 10/2048 [00:00<00:25, 78.55it/s]\n",
      "generating coarse: 100%|██████████| 512/512 [00:14<00:00, 34.83it/s]\n",
      "generating fine: 100%|██████████| 512/512 [02:56<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Everything together\n",
    "audiolm = AudioLM(\n",
    "    wav2vec = wav2vec,\n",
    "    codec = soundstream,\n",
    "    semantic_transformer = semantic_transformer,\n",
    "    coarse_transformer = coarse_transformer,\n",
    "    fine_transformer = fine_transformer\n",
    ")\n",
    "\n",
    "generated_wav = audiolm(batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4rQPHTSRngEr"
   },
   "outputs": [],
   "source": [
    "output_path = \"out.wav\"\n",
    "sample_rate = 44100\n",
    "torchaudio.save(output_path, generated_wav.cpu(), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "is9wLY_ncDYK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
